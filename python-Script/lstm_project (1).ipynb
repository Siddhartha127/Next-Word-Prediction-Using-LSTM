{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3PfOSV5bUe7C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "IrmA3eaj3kF0"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
        "from nltk.corpus import stopwords\n",
        "import pickle\n",
        "import re\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK data (you need to run this once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0u2pdj942Yh",
        "outputId": "49e7291b-ceae-4557-f313-1cfa234e7379"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the text data\n",
        "file_path = \"/content/metamorphosis_clean.txt\"\n",
        "with open(file_path, \"r\", encoding=\"utf8\") as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Convert the list of lines into a pandas DataFrame for easier handling\n",
        "df = pd.DataFrame(lines, columns=[\"Text\"])"
      ],
      "metadata": {
        "id": "yioBz2C4173h"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first and last lines\n",
        "print(f\"First Line: {df['Text'].iloc[0]}\")\n",
        "print(f\"Last Line: {df['Text'].iloc[-1]}\")"
      ],
      "metadata": {
        "id": "KhtDxwL_AXFj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f205d978-8bf5-4fa6-941d-6e95b5874609"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Line: One morning, when Gregor Samsa woke from troubled dreams, he found\n",
            "\n",
            "Last Line: first to get up and stretch out her young body.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all lines into one single string for processing\n",
        "data = ' '.join(df['Text'].values)"
      ],
      "metadata": {
        "id": "K8MRFre9AaG9"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Text Preprocessing\n",
        "def clean_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "    text = text.translate(translator)\n",
        "\n",
        "    # Replace multiple spaces with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "YrpAl3EDAgvh"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply cleaning\n",
        "data = clean_text(data)"
      ],
      "metadata": {
        "id": "44VahqKdAjr9"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization using NLTK\n",
        "tokens = nltk.word_tokenize(data)"
      ],
      "metadata": {
        "id": "UyqwPDzNA5mR"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [word for word in tokens if word not in stop_words]"
      ],
      "metadata": {
        "id": "CrzbvUUQCXPU"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert tokens back to a clean string\n",
        "clean_data = ' '.join(tokens)"
      ],
      "metadata": {
        "id": "qVI0-UUrIsd3"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization and sequences using Keras Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([clean_data])"
      ],
      "metadata": {
        "id": "lXrYHTDFI3uE"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the tokenizer for later use\n",
        "with open('tokenizer.pkl', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle)"
      ],
      "metadata": {
        "id": "kmsFnHx1Qdow"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the text into sequences of integers\n",
        "sequence_data = tokenizer.texts_to_sequences([clean_data])[0]"
      ],
      "metadata": {
        "id": "-wyYqYgZSeck"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(f\"Vocabulary Size: {vocab_size}\")"
      ],
      "metadata": {
        "id": "5OL3vrEXSs_s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bf976ba-3e59-40d1-fcb2-0ac4b7924c59"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 2437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating sequences and preparing input-output pairs for the model\n",
        "def create_sequences(sequence_data):\n",
        "    sequences = []\n",
        "    for i in range(1, len(sequence_data)):\n",
        "        words = sequence_data[i-1:i+1]\n",
        "        sequences.append(words)\n",
        "    return np.array(sequences)"
      ],
      "metadata": {
        "id": "rs1NPitwSgzk"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating sequences from the tokenized data\n",
        "sequences = create_sequences(sequence_data)"
      ],
      "metadata": {
        "id": "iQMJ0I6xSiZf"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split sequences into input (X) and output (y)\n",
        "X = sequences[:, 0]\n",
        "y = sequences[:, 1]"
      ],
      "metadata": {
        "id": "9kVeTvR2S8Fk"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert output (y) to categorical format\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=vocab_size)"
      ],
      "metadata": {
        "id": "chaptT5b5b3J"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape X to add a dimension for timesteps (1 in this case)\n",
        "# X = np.reshape(X, (X.shape[0], 1, 1))"
      ],
      "metadata": {
        "id": "QhiMAf9D6FKI"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model building\n",
        "model = Sequential()\n",
        "\n",
        "# Define input length explicitly for the embedding layer\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=10, input_length=1))  # input_length=1 because each sequence is 1 word\n",
        "\n",
        "# Add LSTM layers\n",
        "model.add(LSTM(512, return_sequences=False))  # The LSTM layer expects 3D input: (batch_size, timesteps, features)\n",
        "\n",
        "# Dense layers\n",
        "model.add(Dense(512, activation=\"relu\"))\n",
        "model.add(Dense(vocab_size, activation=\"softmax\"))\n",
        "\n",
        "# Now the model is built properly\n",
        "model.build(input_shape=(None, 1))  # Define the input shape explicitly\n",
        "\n",
        "# Model summary to see the layer details\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "nqQ8aISk5e2Z",
        "outputId": "bf718cf7-b6f2-4a4b-9009-f3d80167a878"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m)               │          \u001b[38;5;34m24,370\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_9 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │       \u001b[38;5;34m1,071,104\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │         \u001b[38;5;34m262,656\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2437\u001b[0m)                │       \u001b[38;5;34m1,250,181\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">24,370</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,071,104</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2437</span>)                │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,250,181</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,608,311\u001b[0m (9.95 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,608,311</span> (9.95 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,608,311\u001b[0m (9.95 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,608,311</span> (9.95 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "izGVWngU5kDw",
        "outputId": "d870144b-5c3d-460d-b132-d95c799a2e47"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m)               │          \u001b[38;5;34m24,370\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_9 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │       \u001b[38;5;34m1,071,104\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │         \u001b[38;5;34m262,656\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2437\u001b[0m)                │       \u001b[38;5;34m1,250,181\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">24,370</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,071,104</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2437</span>)                │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,250,181</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,608,311\u001b[0m (9.95 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,608,311</span> (9.95 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,608,311\u001b[0m (9.95 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,608,311</span> (9.95 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up callbacks with the updated .keras extension for model saving\n",
        "checkpoint = ModelCheckpoint(\"nextword_model.keras\", monitor='loss', verbose=1, save_best_only=True, mode='auto')\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose=1)\n",
        "\n",
        "logdir = \"logs_next_word\"\n",
        "tensorboard_callback = TensorBoard(log_dir=logdir)\n"
      ],
      "metadata": {
        "id": "SocMmNGO5mkY"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n"
      ],
      "metadata": {
        "id": "mknA_wJH52HY"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(X, y, epochs=100, batch_size=64, callbacks=[checkpoint, reduce_lr, tensorboard_callback])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssI24W7t55qA",
        "outputId": "e8418aef-3913-463c-cfa7-831a7c64c088"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m155/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.5109\n",
            "Epoch 1: loss improved from inf to 7.29951, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 7.5069 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m155/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.8547\n",
            "Epoch 2: loss improved from 7.29951 to 6.89595, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 6.8554 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m152/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.8077\n",
            "Epoch 3: loss improved from 6.89595 to 6.82319, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.8083 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m154/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.7440\n",
            "Epoch 4: loss improved from 6.82319 to 6.77290, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.7447 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m153/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.6485\n",
            "Epoch 5: loss improved from 6.77290 to 6.68174, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.6496 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m153/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.5803\n",
            "Epoch 6: loss improved from 6.68174 to 6.59190, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.5806 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m154/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.5212\n",
            "Epoch 7: loss improved from 6.59190 to 6.52611, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.5213 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m153/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.4704\n",
            "Epoch 8: loss improved from 6.52611 to 6.45893, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.4700 - learning_rate: 0.0010\n",
            "Epoch 9/100\n",
            "\u001b[1m156/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.3701\n",
            "Epoch 9: loss improved from 6.45893 to 6.39023, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 6.3703 - learning_rate: 0.0010\n",
            "Epoch 10/100\n",
            "\u001b[1m152/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 6.3261\n",
            "Epoch 10: loss improved from 6.39023 to 6.32665, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 6.3261 - learning_rate: 0.0010\n",
            "Epoch 11/100\n",
            "\u001b[1m154/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.2217\n",
            "Epoch 11: loss improved from 6.32665 to 6.26685, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 6.2229 - learning_rate: 0.0010\n",
            "Epoch 12/100\n",
            "\u001b[1m153/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.1435\n",
            "Epoch 12: loss improved from 6.26685 to 6.20119, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.1453 - learning_rate: 0.0010\n",
            "Epoch 13/100\n",
            "\u001b[1m150/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.1090\n",
            "Epoch 13: loss improved from 6.20119 to 6.12900, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.1100 - learning_rate: 0.0010\n",
            "Epoch 14/100\n",
            "\u001b[1m152/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.0032\n",
            "Epoch 14: loss improved from 6.12900 to 6.05824, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.0053 - learning_rate: 0.0010\n",
            "Epoch 15/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.9313\n",
            "Epoch 15: loss improved from 6.05824 to 5.98447, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 5.9317 - learning_rate: 0.0010\n",
            "Epoch 16/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.8978\n",
            "Epoch 16: loss improved from 5.98447 to 5.91239, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 5.8979 - learning_rate: 0.0010\n",
            "Epoch 17/100\n",
            "\u001b[1m155/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.7982\n",
            "Epoch 17: loss improved from 5.91239 to 5.83652, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 5.7990 - learning_rate: 0.0010\n",
            "Epoch 18/100\n",
            "\u001b[1m152/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.7185\n",
            "Epoch 18: loss improved from 5.83652 to 5.76126, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 5.7201 - learning_rate: 0.0010\n",
            "Epoch 19/100\n",
            "\u001b[1m153/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.6141\n",
            "Epoch 19: loss improved from 5.76126 to 5.68618, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 5.6164 - learning_rate: 0.0010\n",
            "Epoch 20/100\n",
            "\u001b[1m151/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.5652\n",
            "Epoch 20: loss improved from 5.68618 to 5.60667, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.5670 - learning_rate: 0.0010\n",
            "Epoch 21/100\n",
            "\u001b[1m153/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5.4786\n",
            "Epoch 21: loss improved from 5.60667 to 5.51221, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 5.4797 - learning_rate: 0.0010\n",
            "Epoch 22/100\n",
            "\u001b[1m153/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.3940\n",
            "Epoch 22: loss improved from 5.51221 to 5.41512, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 5.3947 - learning_rate: 0.0010\n",
            "Epoch 23/100\n",
            "\u001b[1m149/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.2658\n",
            "Epoch 23: loss improved from 5.41512 to 5.31299, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 5.2684 - learning_rate: 0.0010\n",
            "Epoch 24/100\n",
            "\u001b[1m149/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.1451\n",
            "Epoch 24: loss improved from 5.31299 to 5.21076, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 5.1487 - learning_rate: 0.0010\n",
            "Epoch 25/100\n",
            "\u001b[1m149/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.0336\n",
            "Epoch 25: loss improved from 5.21076 to 5.10288, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 5.0373 - learning_rate: 0.0010\n",
            "Epoch 26/100\n",
            "\u001b[1m150/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.9184\n",
            "Epoch 26: loss improved from 5.10288 to 4.99220, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 4.9220 - learning_rate: 0.0010\n",
            "Epoch 27/100\n",
            "\u001b[1m151/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.7770\n",
            "Epoch 27: loss improved from 4.99220 to 4.88011, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 4.7815 - learning_rate: 0.0010\n",
            "Epoch 28/100\n",
            "\u001b[1m151/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.6552\n",
            "Epoch 28: loss improved from 4.88011 to 4.76660, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 4.6602 - learning_rate: 0.0010\n",
            "Epoch 29/100\n",
            "\u001b[1m152/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.5831\n",
            "Epoch 29: loss improved from 4.76660 to 4.65751, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 4.5858 - learning_rate: 0.0010\n",
            "Epoch 30/100\n",
            "\u001b[1m154/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 4.4590\n",
            "Epoch 30: loss improved from 4.65751 to 4.55069, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 4.4613 - learning_rate: 0.0010\n",
            "Epoch 31/100\n",
            "\u001b[1m150/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.3752\n",
            "Epoch 31: loss improved from 4.55069 to 4.44058, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 4.3783 - learning_rate: 0.0010\n",
            "Epoch 32/100\n",
            "\u001b[1m150/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.2222\n",
            "Epoch 32: loss improved from 4.44058 to 4.33666, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 4.2279 - learning_rate: 0.0010\n",
            "Epoch 33/100\n",
            "\u001b[1m151/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.1270\n",
            "Epoch 33: loss improved from 4.33666 to 4.23222, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 4.1315 - learning_rate: 0.0010\n",
            "Epoch 34/100\n",
            "\u001b[1m152/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.0361\n",
            "Epoch 34: loss improved from 4.23222 to 4.14154, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 4.0400 - learning_rate: 0.0010\n",
            "Epoch 35/100\n",
            "\u001b[1m149/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.9304\n",
            "Epoch 35: loss improved from 4.14154 to 4.04625, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3.9366 - learning_rate: 0.0010\n",
            "Epoch 36/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.8371\n",
            "Epoch 36: loss improved from 4.04625 to 3.96306, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3.8379 - learning_rate: 0.0010\n",
            "Epoch 37/100\n",
            "\u001b[1m152/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.7774\n",
            "Epoch 37: loss improved from 3.96306 to 3.87804, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3.7811 - learning_rate: 0.0010\n",
            "Epoch 38/100\n",
            "\u001b[1m150/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.7129\n",
            "Epoch 38: loss improved from 3.87804 to 3.80182, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 3.7173 - learning_rate: 0.0010\n",
            "Epoch 39/100\n",
            "\u001b[1m155/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.6592\n",
            "Epoch 39: loss improved from 3.80182 to 3.72803, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 3.6605 - learning_rate: 0.0010\n",
            "Epoch 40/100\n",
            "\u001b[1m152/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.5813\n",
            "Epoch 40: loss improved from 3.72803 to 3.66723, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 3.5846 - learning_rate: 0.0010\n",
            "Epoch 41/100\n",
            "\u001b[1m150/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.4769\n",
            "Epoch 41: loss improved from 3.66723 to 3.60247, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3.4832 - learning_rate: 0.0010\n",
            "Epoch 42/100\n",
            "\u001b[1m152/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.4317\n",
            "Epoch 42: loss improved from 3.60247 to 3.54251, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3.4359 - learning_rate: 0.0010\n",
            "Epoch 43/100\n",
            "\u001b[1m152/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.3856\n",
            "Epoch 43: loss improved from 3.54251 to 3.49357, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3.3897 - learning_rate: 0.0010\n",
            "Epoch 44/100\n",
            "\u001b[1m149/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.3229\n",
            "Epoch 44: loss improved from 3.49357 to 3.43970, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3.3295 - learning_rate: 0.0010\n",
            "Epoch 45/100\n",
            "\u001b[1m153/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.3175\n",
            "Epoch 45: loss improved from 3.43970 to 3.40234, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3.3202 - learning_rate: 0.0010\n",
            "Epoch 46/100\n",
            "\u001b[1m150/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.2254\n",
            "Epoch 46: loss improved from 3.40234 to 3.35078, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3.2316 - learning_rate: 0.0010\n",
            "Epoch 47/100\n",
            "\u001b[1m155/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.2045\n",
            "Epoch 47: loss improved from 3.35078 to 3.31755, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3.2066 - learning_rate: 0.0010\n",
            "Epoch 48/100\n",
            "\u001b[1m153/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.1667\n",
            "Epoch 48: loss improved from 3.31755 to 3.27769, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 3.1701 - learning_rate: 0.0010\n",
            "Epoch 49/100\n",
            "\u001b[1m153/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.1168\n",
            "Epoch 49: loss improved from 3.27769 to 3.24124, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 3.1207 - learning_rate: 0.0010\n",
            "Epoch 50/100\n",
            "\u001b[1m156/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.0892\n",
            "Epoch 50: loss improved from 3.24124 to 3.21310, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 3.0908 - learning_rate: 0.0010\n",
            "Epoch 51/100\n",
            "\u001b[1m149/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.0791\n",
            "Epoch 51: loss improved from 3.21310 to 3.17558, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3.0845 - learning_rate: 0.0010\n",
            "Epoch 52/100\n",
            "\u001b[1m155/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.0382\n",
            "Epoch 52: loss improved from 3.17558 to 3.14868, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3.0403 - learning_rate: 0.0010\n",
            "Epoch 53/100\n",
            "\u001b[1m152/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.9745\n",
            "Epoch 53: loss improved from 3.14868 to 3.11486, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.9798 - learning_rate: 0.0010\n",
            "Epoch 54/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.9581\n",
            "Epoch 54: loss improved from 3.11486 to 3.09141, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.9590 - learning_rate: 0.0010\n",
            "Epoch 55/100\n",
            "\u001b[1m149/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.9503\n",
            "Epoch 55: loss improved from 3.09141 to 3.06773, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.9570 - learning_rate: 0.0010\n",
            "Epoch 56/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.8942\n",
            "Epoch 56: loss improved from 3.06773 to 3.03599, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 2.8951 - learning_rate: 0.0010\n",
            "Epoch 57/100\n",
            "\u001b[1m154/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.8919\n",
            "Epoch 57: loss improved from 3.03599 to 3.01925, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.8951 - learning_rate: 0.0010\n",
            "Epoch 58/100\n",
            "\u001b[1m149/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.8713\n",
            "Epoch 58: loss improved from 3.01925 to 2.99525, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2.8783 - learning_rate: 0.0010\n",
            "Epoch 59/100\n",
            "\u001b[1m149/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.8458\n",
            "Epoch 59: loss improved from 2.99525 to 2.97370, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.8530 - learning_rate: 0.0010\n",
            "Epoch 60/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.8462\n",
            "Epoch 60: loss improved from 2.97370 to 2.95388, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.8469 - learning_rate: 0.0010\n",
            "Epoch 61/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.8357\n",
            "Epoch 61: loss improved from 2.95388 to 2.93421, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.8363 - learning_rate: 0.0010\n",
            "Epoch 62/100\n",
            "\u001b[1m151/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.8059\n",
            "Epoch 62: loss improved from 2.93421 to 2.91600, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.8106 - learning_rate: 0.0010\n",
            "Epoch 63/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.7604\n",
            "Epoch 63: loss improved from 2.91600 to 2.89868, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.7613 - learning_rate: 0.0010\n",
            "Epoch 64/100\n",
            "\u001b[1m154/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.7510\n",
            "Epoch 64: loss improved from 2.89868 to 2.88438, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 2.7544 - learning_rate: 0.0010\n",
            "Epoch 65/100\n",
            "\u001b[1m153/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.7325\n",
            "Epoch 65: loss improved from 2.88438 to 2.86268, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 2.7366 - learning_rate: 0.0010\n",
            "Epoch 66/100\n",
            "\u001b[1m151/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.7246\n",
            "Epoch 66: loss improved from 2.86268 to 2.85127, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2.7301 - learning_rate: 0.0010\n",
            "Epoch 67/100\n",
            "\u001b[1m155/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.7450\n",
            "Epoch 67: loss improved from 2.85127 to 2.83776, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.7468 - learning_rate: 0.0010\n",
            "Epoch 68/100\n",
            "\u001b[1m150/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.7197\n",
            "Epoch 68: loss improved from 2.83776 to 2.82161, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.7248 - learning_rate: 0.0010\n",
            "Epoch 69/100\n",
            "\u001b[1m149/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6831\n",
            "Epoch 69: loss improved from 2.82161 to 2.80971, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.6901 - learning_rate: 0.0010\n",
            "Epoch 70/100\n",
            "\u001b[1m152/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6829\n",
            "Epoch 70: loss improved from 2.80971 to 2.79195, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.6871 - learning_rate: 0.0010\n",
            "Epoch 71/100\n",
            "\u001b[1m155/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6441\n",
            "Epoch 71: loss improved from 2.79195 to 2.78126, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.6467 - learning_rate: 0.0010\n",
            "Epoch 72/100\n",
            "\u001b[1m149/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6652\n",
            "Epoch 72: loss improved from 2.78126 to 2.77325, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.6712 - learning_rate: 0.0010\n",
            "Epoch 73/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6647\n",
            "Epoch 73: loss improved from 2.77325 to 2.76060, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.6653 - learning_rate: 0.0010\n",
            "Epoch 74/100\n",
            "\u001b[1m153/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.6496\n",
            "Epoch 74: loss improved from 2.76060 to 2.74678, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 2.6527 - learning_rate: 0.0010\n",
            "Epoch 75/100\n",
            "\u001b[1m151/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6369\n",
            "Epoch 75: loss improved from 2.74678 to 2.73482, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 2.6412 - learning_rate: 0.0010\n",
            "Epoch 76/100\n",
            "\u001b[1m149/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6028\n",
            "Epoch 76: loss improved from 2.73482 to 2.72461, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.6096 - learning_rate: 0.0010\n",
            "Epoch 77/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6101\n",
            "Epoch 77: loss improved from 2.72461 to 2.72001, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.6108 - learning_rate: 0.0010\n",
            "Epoch 78/100\n",
            "\u001b[1m154/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6075\n",
            "Epoch 78: loss improved from 2.72001 to 2.70675, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.6100 - learning_rate: 0.0010\n",
            "Epoch 79/100\n",
            "\u001b[1m155/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6056\n",
            "Epoch 79: loss improved from 2.70675 to 2.69655, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.6073 - learning_rate: 0.0010\n",
            "Epoch 80/100\n",
            "\u001b[1m152/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6183\n",
            "Epoch 80: loss improved from 2.69655 to 2.68706, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.6208 - learning_rate: 0.0010\n",
            "Epoch 81/100\n",
            "\u001b[1m150/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.5591\n",
            "Epoch 81: loss improved from 2.68706 to 2.67977, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.5652 - learning_rate: 0.0010\n",
            "Epoch 82/100\n",
            "\u001b[1m150/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.5429\n",
            "Epoch 82: loss improved from 2.67977 to 2.66988, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.5492 - learning_rate: 0.0010\n",
            "Epoch 83/100\n",
            "\u001b[1m151/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5512\n",
            "Epoch 83: loss improved from 2.66988 to 2.66562, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 2.5562 - learning_rate: 0.0010\n",
            "Epoch 84/100\n",
            "\u001b[1m154/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.5326\n",
            "Epoch 84: loss improved from 2.66562 to 2.65591, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 2.5357 - learning_rate: 0.0010\n",
            "Epoch 85/100\n",
            "\u001b[1m153/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.5497\n",
            "Epoch 85: loss improved from 2.65591 to 2.64975, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 2.5528 - learning_rate: 0.0010\n",
            "Epoch 86/100\n",
            "\u001b[1m156/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.5156\n",
            "Epoch 86: loss improved from 2.64975 to 2.64596, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 2.5172 - learning_rate: 0.0010\n",
            "Epoch 87/100\n",
            "\u001b[1m156/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.5244\n",
            "Epoch 87: loss improved from 2.64596 to 2.63563, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.5258 - learning_rate: 0.0010\n",
            "Epoch 88/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.5034\n",
            "Epoch 88: loss improved from 2.63563 to 2.62838, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.5042 - learning_rate: 0.0010\n",
            "Epoch 89/100\n",
            "\u001b[1m150/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.5063\n",
            "Epoch 89: loss improved from 2.62838 to 2.62124, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.5121 - learning_rate: 0.0010\n",
            "Epoch 90/100\n",
            "\u001b[1m150/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.4912\n",
            "Epoch 90: loss improved from 2.62124 to 2.61202, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.4971 - learning_rate: 0.0010\n",
            "Epoch 91/100\n",
            "\u001b[1m150/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.4980\n",
            "Epoch 91: loss improved from 2.61202 to 2.61125, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.5036 - learning_rate: 0.0010\n",
            "Epoch 92/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.5021\n",
            "Epoch 92: loss improved from 2.61125 to 2.60010, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.5027 - learning_rate: 0.0010\n",
            "Epoch 93/100\n",
            "\u001b[1m156/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.4901\n",
            "Epoch 93: loss improved from 2.60010 to 2.59361, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 2.4914 - learning_rate: 0.0010\n",
            "Epoch 94/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.4833\n",
            "Epoch 94: loss improved from 2.59361 to 2.59150, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 2.4840 - learning_rate: 0.0010\n",
            "Epoch 95/100\n",
            "\u001b[1m154/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.4730\n",
            "Epoch 95: loss improved from 2.59150 to 2.58899, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 2.4759 - learning_rate: 0.0010\n",
            "Epoch 96/100\n",
            "\u001b[1m152/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.4735\n",
            "Epoch 96: loss improved from 2.58899 to 2.58078, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.4776 - learning_rate: 0.0010\n",
            "Epoch 97/100\n",
            "\u001b[1m150/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.4638\n",
            "Epoch 97: loss improved from 2.58078 to 2.57459, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.4694 - learning_rate: 0.0010\n",
            "Epoch 98/100\n",
            "\u001b[1m156/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.4394\n",
            "Epoch 98: loss improved from 2.57459 to 2.57019, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.4410 - learning_rate: 0.0010\n",
            "Epoch 99/100\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.4501\n",
            "Epoch 99: loss improved from 2.57019 to 2.56812, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.4509 - learning_rate: 0.0010\n",
            "Epoch 100/100\n",
            "\u001b[1m153/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.4399\n",
            "Epoch 100: loss improved from 2.56812 to 2.55792, saving model to nextword_model.keras\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.4436 - learning_rate: 0.0010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r6ZelXPxCr-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import pickle\n",
        "\n",
        "# Load the trained model (use the path where your model is saved)\n",
        "model = load_model('nextword_model.keras')\n",
        "\n",
        "# Load the tokenizer you previously saved\n",
        "with open('tokenizer.pkl', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n"
      ],
      "metadata": {
        "id": "ARdsTQLp-dKh"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Input sentence (you can change this)\n",
        "input_text = \"at the dull\"\n",
        "\n",
        "# Tokenize the input text\n",
        "input_sequence = tokenizer.texts_to_sequences([input_text])[0]\n",
        "\n",
        "# Padding or adjusting sequence length (if your model expects a specific input length)\n",
        "# For next-word prediction, you might not need padding as you're predicting one word at a time\n",
        "input_sequence = np.array(input_sequence[-1:]).reshape(1, 1)  # Take the last word and reshape for the model\n"
      ],
      "metadata": {
        "id": "yqyvx5Nz_qpo"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the next word\n",
        "predicted_probs = model.predict(input_sequence)\n",
        "\n",
        "# Get the word index with the highest probability\n",
        "predicted_word_index = np.argmax(predicted_probs, axis=-1)[0]\n",
        "\n",
        "# Convert the predicted word index back to the actual word using the tokenizer's word index\n",
        "predicted_word = tokenizer.index_word[predicted_word_index]\n",
        "\n",
        "print(f\"Next word prediction: {predicted_word}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwJ2PbK7_whA",
        "outputId": "09a09bf6-901c-4089-bd08-1de740fc86b4"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Next word prediction: weather\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text, next_words, model, tokenizer):\n",
        "    for _ in range(next_words):\n",
        "        # Tokenize the current seed text\n",
        "        input_sequence = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        input_sequence = np.array(input_sequence[-1:]).reshape(1, 1)\n",
        "\n",
        "        # Predict the next word\n",
        "        predicted_probs = model.predict(input_sequence)\n",
        "        predicted_word_index = np.argmax(predicted_probs, axis=-1)[0]\n",
        "\n",
        "        # Get the predicted word\n",
        "        predicted_word = tokenizer.index_word[predicted_word_index]\n",
        "\n",
        "        # Append the word to the seed text\n",
        "        seed_text += ' ' + predicted_word\n",
        "\n",
        "    return seed_text\n",
        "\n",
        "# Example of generating 10 words after the seed text\n",
        "output_text = generate_text(\"Gregor then turned to look\", 10, model, tokenizer)\n",
        "print(output_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bygDG6zL_8wQ",
        "outputId": "2e9f05c9-a575-4a49-a56c-3ad0be3ba94e"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Gregor then turned to look while she other let turn panting to would had and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Step 1: Evaluate the model on the training set (you can also use a validation/test set if you have one)\n",
        "loss = model.evaluate(X, y)\n",
        "print(f\"Model Loss: {loss}\")\n",
        "\n",
        "# Step 2: Visualize the training history\n",
        "# Plot loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "# Plot accuracy if you have it (this assumes you added an accuracy metric when compiling the model)\n",
        "if 'accuracy' in history.history:\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "BYdyaHDdCPIi",
        "outputId": "73c58f50-8209-446b-c63a-caadeb6ba09b"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.3226\n",
            "Model Loss: 2.3502135276794434\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}